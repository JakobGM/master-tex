@book{dive-into-deep-learning,
  title={Dive into Deep Learning},
  author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  edition={Release 0.7.0},
  url={https://d2l.ai},
  urldate={2019-11-28}
}

% First book in Visual Intelligence syllabus
@misc{visint-cnn,
  publisher = {Pearson},
  isbn = {9781292223049},
  year = {2018},
  title = {Digital Image Processing},
  edition = {4th ed.},
  language = {eng},
  address = {New York},
  author = {Gonzalez, Rafael C},
  keywords = {Digital bildebehandling; Bildebehandling; digital bildebehandling},
}

% --- Activation functions ---
@book{goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
% Original "Universal Approximation Theorem" paper, using sigmoid function.
@article{uat-sigmoid,
  title={Approximations by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals and Systems},
  volume={2},
  pages={183--192},
  year={1989}
}
% Follow-up "Universal Approximation Theorem" paper, using nonpolynomial activation functions
@article{uat-nonpolynomial,
  title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  journal={Neural networks},
  volume={6},
  number={6},
  pages={861--867},
  year={1993},
  publisher={Elsevier}
}
% Original artificial perceptron paper
% https://psycnet.apa.org/fulltext/1959-09865-001.pdf
@article{rosenblatt-perceptron-1958,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}
% Original ReLU paper
% https://www.nature.com/articles/35016072
@article{relu-original-paper,
  title={Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
  author={Hahnloser, Richard HR and Sarpeshkar, Rahul and Mahowald, Misha A and Douglas, Rodney J and Seung, H Sebastian},
  journal={Nature},
  volume={405},
  number={6789},
  pages={947},
  year={2000},
  publisher={Nature Publishing Group}
}
% Popularity of ReLU function noted on page 438 in lower right
% Gives a good overview of several deep learning topics, including CNNs
% https://www.nature.com/articles/nature14539.pdf
@article{relu-popularity,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}
% Explains how ReLU activation function is an improvement over sigmoid
% http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf
@inproceedings{relu-better-than-sigmoid,
  title={Deep sparse rectifier neural networks},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={315--323},
  year={2011}
}
