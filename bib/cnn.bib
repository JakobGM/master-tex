@book{dive-into-deep-learning,
  title={Dive into Deep Learning},
  author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  edition={Release 0.7.0},
  url={https://d2l.ai},
  urldate={2019-11-28}
}

% First book in Visual Intelligence syllabus
@misc{visint-cnn,
  publisher = {Pearson},
  isbn = {9781292223049},
  year = {2018},
  title = {Digital Image Processing},
  edition = {4th ed.},
  language = {eng},
  address = {New York},
  author = {Gonzalez, Rafael C},
  keywords = {Digital bildebehandling; Bildebehandling; digital bildebehandling},
}

% --- Activation functions ---
@book{goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
% Original "Universal Approximation Theorem" paper, using sigmoid function.
@article{uat-sigmoid,
  title={Approximations by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals and Systems},
  volume={2},
  pages={183--192},
  year={1989}
}
% Follow-up "Universal Approximation Theorem" paper, using nonpolynomial activation functions
@article{uat-nonpolynomial,
  title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  journal={Neural networks},
  volume={6},
  number={6},
  pages={861--867},
  year={1993},
  publisher={Elsevier}
}
% Original artificial perceptron paper
% https://psycnet.apa.org/fulltext/1959-09865-001.pdf
@article{rosenblatt-perceptron-1958,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}
% Original ReLU paper
% https://www.nature.com/articles/35016072
@article{relu-original-paper,
  title={Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
  author={Hahnloser, Richard HR and Sarpeshkar, Rahul and Mahowald, Misha A and Douglas, Rodney J and Seung, H Sebastian},
  journal={Nature},
  volume={405},
  number={6789},
  pages={947},
  year={2000},
  publisher={Nature Publishing Group}
}
% Popularity of ReLU function noted on page 438 in lower right
% Gives a good overview of several deep learning topics, including CNNs
% https://www.nature.com/articles/nature14539.pdf
@article{relu-popularity,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}
% Explains how ReLU activation function is an improvement over sigmoid
% http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf
@inproceedings{relu-better-than-sigmoid,
  title={Deep sparse rectifier neural networks},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={315--323},
  year={2011}
}

% --- Pooling ---
% First book in Visual Intelligence syllabus
@misc{cnn-translational-invariance,
  title={Quantifying Translation-Invariance in Convolutional Neural Networks},
  author={Eric Kauderer-Abrams},
  year={2017},
  eprint={1801.01450},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

% --- Batch normalization ---
@misc{batch-normalization,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Sergey Ioffe and Christian Szegedy},
  year={2015},
  eprint={1502.03167},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

% --- Dropout ---
% Can be used to cite the introduction of dropout as a concept
% https://arxiv.org/pdf/1207.0580.pdf
@article{dropout-original-paper,
    title={Improving neural networks by preventing co-adaptation of feature detectors},
    author={Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
    year={2012},
    eprint={1207.0580},
    journal={arXiv preprint arXiv:1207.0580},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}
% Overview of dropout techniques. Also describes dropout in CNNs.
% Describes that batch normalization is considered a regularization technique as well.
% https://arxiv.org/pdf/1904.13310.pdf
@article{dropout-cnn,
    title={Survey of Dropout Methods for Deep Neural Networks},
    author={Alex Labach and Hojjat Salehinejad and Shahrokh Valaee},
    year={2019},
    eprint={1904.13310},
    journal={arXiv preprint arXiv:1904.13310},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}
% CNN dropout technique, removing part of input image
% https://arxiv.org/pdf/1708.04552.pdf
@article{dropout-cutout,
    title={Improved Regularization of Convolutional Neural Networks with Cutout},
    author={Terrance DeVries and Graham W. Taylor},
    year={2017},
    eprint={1708.04552},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
% CNN dropout technique, dropping entire layers such that they simple forward their output
@InProceedings{dropout-stochastic-depth,
    author="Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.",
    editor="Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max",
    title="Deep Networks with Stochastic Depth",
    booktitle="Computer Vision -- ECCV 2016",
    year="2016",
    publisher="Springer International Publishing",
    address="Cham",
    pages="646--661",
    isbn="978-3-319-46493-0"
}
% Paper starting work on dropout in convolutional layers, introducing max-pooling dropout,
% a.k.a. probabilistic weighted pooling
% https://www.sciencedirect.com/science/article/pii/S0893608015001446
@article{max-pooling-dropout,
    title = "Towards dropout training for convolutional neural networks",
    journal = "Neural Networks",
    volume = "71",
    pages = "1--10",
    year = "2015",
    issn = "0893-6080",
    doi = "https://doi.org/10.1016/j.neunet.2015.07.007",
    url = "http://www.sciencedirect.com/science/article/pii/S0893608015001446",
    author = "Haibing Wu and Xiaodong Gu",
    keywords = "Deep learning, Convolutional neural networks, Max-pooling dropout"
}
