\begin{wrapfigure}[9]{r}{.39\textwidth}
  \fbox{\begin{minipage}{\dimexpr\linewidth-2\fboxrule-2\fboxsep}
    \begin{flushleft}
      \small
      \begin{center}\textbf{Training summary}\end{center}
      \begin{itemize}[nosep,leftmargin=*]
        \item \num{58559} geographic tiles: \\
          $\SI{256}{\pixel} \times \SI{256}{\pixel} = \SI{64}{\meter} \times \SI{64}{\meter}$.
        \item \numtraintiles / \numvalidationtiles / \numtesttiles \\
          training / validation / test.
        \item Random shuffling.
        \item Adam optimizer.
        \item Validation loss early stopping.
      \end{itemize}
    \end{flushleft}
  \end{minipage}}
\end{wrapfigure}

The Trondheim dataset produces \numtiles geographic tiles after being processed, each tile including aerial photography (RGB) data, elevation data (LiDAR elevation), ground truth semantic roof segmentation masks and surface normal vector rasters.
This sample space is split into a customary 70\% / 15\% / 15\% training--validation--testing split.
The training data is randomly shuffled and subsequently grouped into batches of size 16 before applying the Adam optimizer.
Training is continued until observed convergence by the use of the loss evaluated over the validation split.
The weights corresponding to the epoch yielding the best validation loss is used as the final model parametrization.
