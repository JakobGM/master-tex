We now intend to construct and train a multitask learning (MTL) model which simultaneously predicts semantic roof segmentation masks \emph{and} surface normal vector rasters.
The main issue is constructing a proper loss function which can be optimized such that \emph{both} tasks are solved simultaneously to a satisfactory degree.
We construct a simple multitask loss function, $\mtloss$, which is a simply a linear combination of the soft Jaccard loss, as defined in \cref{eq:soft-jaccard-loss}, and the pixel-averaged cosine similarity loss, as defined in \cref{eq:cosine-loss}, yielding the following multitask loss function,
\begin{align*}
  \mtlossf
  &\defeq
  \alpha \cdot \seglossf
  +
  (1 - \alpha) \cdot \normlossf
  \\
  &=
  \alpha - \frac{%
    \alpha \cdot \sum\limits_{i = 1}^{HW}
    \predsegraster_i \segraster_i
  }{%
    \sum\limits_{i = 1}^{HW} \left(
      \predsegraster_i
      +
      \segraster_i
      -
      \predsegraster_i \segraster_i
    \right)
  }
  +
  \frac{1 - \alpha}{HW}
  \sum_{i = 1}^{H} \sum_{j = 1}^{W} \left(
    1 - \normraster_{i,j}^T~\prednormraster_{i,j}
  \right).
\end{align*}
%
where $\alpha \in [0, 1]$ determines the weighting of the linear combination.
The main challenge is now to select a suitable value for $\alpha$, too small and the semantic segmentation task is neglected during optimization, and too close to 1 and the optimization ends up neglecting the task of predicting surface normal vectors.
In order to find a suitable value for $\alpha$, we train several different models for 20 epochs and evaluate the resulting models, i.e.\ perform a hyperparameter search for $\alpha$.
We will specifically train models with the following weightings: $\alpha \in \{0, 10^{-8}, 10^{-7}, 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 0.5, 1\}$.
Here $\alpha = 0$ implies a single-task model which only predicts surface normal vectors, and $\alpha = 0$ implies a single-task model which only predicts semantic roof segmentation maps.

The value decomposition of the multitask loss function, $\mtloss$, into the two linear components, $\segloss$ and $\normloss$, evaluated on the training split during 20 epochs of training is presented in \cref{fig:alpha-train-losses}.
Likewise, the value decomposition of the multitask loss function evaluated over the validation split is presented in \cref{fig:alpha-validation-losses}.
\begin{figure}
  \includegraphics[width=\linewidth]{multitask-alpha-train.pdf}
  \appcaption{%
    End-of-epoch \emph{training} loss decomposition for different values of $\alpha$ over 20 training epochs.
  }{%
    The training epochs are given along the horizontal axis, while the end-of-epoch loss values are given along the vertical axis.
    The top plot shows the soft Jaccard loss, $\segloss$, while the bottom plot shows the batch-summed, pixel-averaged cosine similarity loss, $\normloss$.
    The value for $\alpha$ used during training is indicated by the color of the line as described in the right-hand color legend.
  }%
  \label{fig:alpha-train-losses}
\end{figure}
%
\begin{figure}
  \includegraphics[width=\linewidth]{multitask-alpha-validation.pdf}
  \appcaption{%
    End-of-epoch \emph{validation} loss decomposition for different values of $\alpha$ over 20 training epochs.
  }{%
    Detailed figure description is provided in \cref{fig:alpha-train-losses}.
  }%
  \label{fig:alpha-validation-losses}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{best-alphas.pdf}
  \appcaption{%
    Best loss value across all 20 training epochs for each value of $\alpha$.
  }{%
    The value of $\alpha$ is provided along the log-scaled horizontal axis, while the best loss values are provided along the vertical axis.
    The top row shows the soft Jaccard loss, $\segloss$, while the bottom row shows the pixel-averages cosine similarity loss, $\normloss$.
    The left column is evaluated on the train split, while the right column is evaluated on the validation split.
  }
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{training/multitask-train+validation-loss.pdf}
  \appcaption{%
    Training procedure of combined input, U-Net-derived multitask architecture ($\alpha = 10^{-5}$) for predicting semantic segmentation masks \emph{and} surface normals over 100 training epochs.
  }{%
    The training epochs are given along the horizontal axis, while the end-of-epoch multitask loss evaluations, $\mtloss$, are given along the vertical axis.
    Validation split loss is shown as a \textcolor{blue}{blue} solid line, while the training split loss is shown as a \textcolor{blue}{blue} dashed line.
    The epoch yielding the best validation loss is annotated as a solid \textcolor{blue}{blue} circle, in this case a validation loss of \num{13.1363}.
  }%
  \label{fig:multitask-training}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{training/multitask+normals_with_lidar_and_rgb-train+validation-surface_normal_loss.pdf}
  \caption{Multitask vs single-task surface normal network (training + validation split).}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{training/multitask+segmentation_with_lidar_and_rgb-train+validation-surface_segmentation_loss.pdf}
  \caption{Multitask vs single-task surface segmentation network (training + validation split).}
\end{figure}

\begin{figure}
  \includegraphics[width=0.75\textwidth]{img/evaluation/multitask/instance-averaged-cosine-similarities.pdf}
  \caption{TODO}
\end{figure}

\begin{figure}
  \includegraphics[width=0.75\textwidth]{img/iou_distribution/segmentation_with_lidar_and_rgb.pdf}
  \includegraphics[width=0.75\textwidth]{img/iou_distribution/multitask.pdf}
  \caption{TODO}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{img/metric_correlation/multitask+segmentation_with_lidar_and_rgb+iou.pdf}
  \caption{TODO}
\end{figure}
